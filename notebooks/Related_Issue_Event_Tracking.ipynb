{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Project on Korea Herald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 23769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title          object\n",
       "author         object\n",
       "time           object\n",
       "description    object\n",
       "body           object\n",
       "section        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "data_path = '../data/koreaherald_1517_#.json.gz'\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "\n",
    "for i in range(8):\n",
    "  p = data_path.replace('#',str(i))\n",
    "  with gzip.open(p,'rb') as f:\n",
    "    data = pd.DataFrame.from_dict(json.load(f))\n",
    "  df_data = df_data.append(data,ignore_index=True)\n",
    "\n",
    "# clean up column names\n",
    "df_data = df_data.rename(columns={\" author\": \"author\",\n",
    "                        \" time\": \"time\",\n",
    "                        \" description\": \"description\",\n",
    "                        \" body\": \"body\",\n",
    "                        \" section\": \"section\",\n",
    "                       })\n",
    "# preview data\n",
    "print('Number of docs: {}'.format(df_data.shape[0]))\n",
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Here we apply:\n",
    "- tokenisation\n",
    "- lemmatisation\n",
    "- normalisation\n",
    "\n",
    "(optional) Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenise_pipeline(doc):\n",
    "  doc = doc.lower()  # Convert to lowercase.\n",
    "  tokens = tokenizer.tokenize(doc) # split into words\n",
    "  # TODO: remove stopwords\n",
    "  tokens = [token for token in tokens if not token.isnumeric()] # remove numbers\n",
    "  tokens = [token for token in tokens if token not in stop_words]\n",
    "  tokens = [token for token in tokens if len(token) > 2] # remove words of only 1 letter\n",
    "  tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatisation\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data['body_tokenised'] = df_data['body'].apply(tokenise_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "docs = [doc for doc in df_data['body_tokenised']]\n",
    "bigrams = Phrases(docs, min_count=20) # keeps only phrases that appear >= 20 times in corpus.\n",
    "\n",
    "def add_bigrams(doc):\n",
    "  for token in bigrams[doc]:\n",
    "    if '_' in token:\n",
    "      doc.append(token)\n",
    "  return doc\n",
    "\n",
    "df_data['body_tokenised'] = df_data['body_tokenised'].apply(add_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 16582\n",
      "Number of documents: 23769\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5) # filter words that occur in less than 20 docs or more than 50% docs\n",
    "\n",
    "print('Number of unique tokens: {}'.format(len(dictionary)))\n",
    "print('Number of documents: {}'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title_bow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title_bow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-823faf9ac47e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_bow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_tokenised'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_bow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_bow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title_bow'"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df_data['body_tokenised']]\n",
    "tfidf = TfidfModel(corpus)\n",
    "df_data['body_bow'] = df_data['body_tokenised'].apply(dictionary.doc2bow)\n",
    "df_data['body_tfidf'] = df_data['body_bow'].apply(lambda doc : tfidf[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "temp = dictionary[0] # to 'load' dictionary\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "# parameters\n",
    "feature = 'body_bow' # lda uses bow\n",
    "corpus = df_data[feature].values\n",
    "num_topics = 20\n",
    "\n",
    "model = LdaModel(\n",
    "  corpus = corpus,\n",
    "  id2word = id2word,\n",
    "  chunksize = 2000,\n",
    "  alpha = 'auto',\n",
    "  eta = 'auto',\n",
    "  iterations = 400,\n",
    "  num_topics = num_topics,\n",
    "  passes = 20,\n",
    "  eval_every = None\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Topics to Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(ls):\n",
    "  return max(ls, key = lambda item: item[1])\n",
    "\n",
    "df_data['topic'] = df_data[feature].apply(lambda x : argmax(model.get_document_topics(x))[0])\n",
    "df_data['topic_confidence'] = df_data[feature].apply(lambda x : argmax(model.get_document_topics(x))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(model.show_topics(num_topics=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "cols = cols * 2\n",
    "\n",
    "max_words = 15\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=max_words,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = model.show_topics(formatted=False,num_topics=num_topics, num_words=max_words)\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "labels = ['ORG', 'LOC', 'PERSON']\n",
    "def get_entities(txt):\n",
    "    doc = nlp(txt)\n",
    "  \n",
    "    #doc now contains all the entities identified\n",
    "    NER_list = [((ent.text),ent.label_) for ent in doc.ents]\n",
    "\n",
    "    #this line is to remove duplicate entities\n",
    "    NER_list = list(dict.fromkeys(NER_list))\n",
    "\n",
    "    #converting list of tuples to list of lists for easier manipulation later\n",
    "    NER_list_final = [list(entity) for entity in NER_list]\n",
    "\n",
    "    #Filtering for the entity labels that we are interested in.\n",
    "    entities = {}\n",
    "    entities[labels[0]] = [entity[0] for entity in NER_list_final if entity[1] == labels[0]]\n",
    "    entities[labels[1]] = [entity[0] for entity in NER_list_final if entity[1] == labels[1]]\n",
    "    entities[labels[2]] = [entity[0] for entity in NER_list_final if entity[1] == labels[2]]\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Issue Event Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Method to extract hypernym of a word\n",
    "def hypernym_extract(string):\n",
    "    string += '.n.01'\n",
    "    hyper = wn.synset(string).hypernyms()\n",
    "    return str(hyper[0])[str(hyper[0]).index('\\'')+1:str(hyper[0]).index('.')]\n",
    "\n",
    "# Method to remove duplicates of elements in a list\n",
    "def unique_ele(alist):\n",
    "    s = set(alist)\n",
    "    alist = list(s)\n",
    "    return alist\n",
    "\n",
    "# Check if all values in ner_dic are present in the body\n",
    "def NER_check(ner_dic, disc_words, score):\n",
    "    ner = list(get_all_values(ner_dic))\n",
    "        \n",
    "    if len(ner) > 0 and len(ner) < 15:\n",
    "        for i in range(len(ner)):\n",
    "            ner[i] = ner[i].lower()\n",
    "        if all(word in disc_words for word in ner):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return score\n",
    "    else:\n",
    "        return score\n",
    "    \n",
    "# Get list of all the values from a dictionary\n",
    "def get_all_values(d):\n",
    "    if isinstance(d, dict):\n",
    "        for v in d.values():\n",
    "            yield from get_all_values(v)\n",
    "    elif isinstance(d, list):\n",
    "        for v in d:\n",
    "            yield from get_all_values(v)\n",
    "    else:\n",
    "        yield d \n",
    "\n",
    "# Extract all discriminative words from each topic to represent an issue\n",
    "issues_all = {}\n",
    "for i in range(len(topics)):\n",
    "    k_words = []\n",
    "    for j in range(max_words):\n",
    "        k_words.append(topics[i][1][j][0])\n",
    "    issues_all[i] = k_words\n",
    "\n",
    "\n",
    "# Extract Top k discriminative words from each topic to represent an issue\n",
    "issues = {}\n",
    "k = 5\n",
    "for i in range(len(topics)):\n",
    "    k_words = []\n",
    "    for j in range(k):\n",
    "        k_words.append(topics[i][1][j][0])\n",
    "    issues[i] = k_words\n",
    "\n",
    "# Append hypernyms of values in issues\n",
    "for i in range(len(issues)):\n",
    "    for j in range(k):\n",
    "        try:\n",
    "            issues[i].append(hypernym_extract(issues[i][j]))\n",
    "        except:\n",
    "            print (\"Hypernym of '{}' is not found.\".format(issues[i][j]))\n",
    "\n",
    "# Remove duplicate words in the list of 'body_tokenised' column\n",
    "df_unique_tokens = df_data.copy()\n",
    "df_unique_tokens['body_tokenised'] = df_unique_tokens['body_tokenised'].apply(unique_ele)\n",
    "\n",
    "# Add NER as columns for the entities recognized\n",
    "df_unique_tokens['NER'] = df_unique_tokens['body'].apply(lambda row : get_entities(row))\n",
    "\n",
    "# Get number of matching words in the whitelist\n",
    "for  i in range(len(issues)):\n",
    "    \n",
    "    # The more same words that a document contains as that in whitelist, \n",
    "    # the higher chance that the document is related to the issue(topic cluster)\n",
    "    whitelist = issues[i]\n",
    "    \n",
    "    # Assign name of the columns which will contain the scores\n",
    "    issue_index = 'issue#'\n",
    "    ind = issue_index.replace('#',str(i))\n",
    "    \n",
    "    # Create a dataframe containing docs outside ith topic cluster\n",
    "    df_docs_outside_cluster = df_unique_tokens[df_unique_tokens['topic']!=i].copy()\n",
    "    # Give score based on the number of words in whitelist appearing in the body\n",
    "    df_unique_tokens[ind] = df_docs_outside_cluster['body_tokenised'].apply(lambda row:len(set(whitelist) & set(row)))\n",
    "    # Give score of NaN to the documents with NER matching all words \n",
    "    # in the ith topic cluster to avoid extracting the same event\n",
    "    df_unique_tokens[ind] = df_unique_tokens.apply(lambda row : NER_check(row['NER'], issues_all[i], row[ind]), axis=1)   \n",
    "    \n",
    "    # Sort and copy top 10 rows\n",
    "    df_temp = df_unique_tokens.sort_values(ind, ascending = False).head(10).copy()\n",
    "    \n",
    "    # Assign title of docuemnts as the name of an event\n",
    "    events = list(df_temp['title'])\n",
    "    # Create a list of NERs of top 10 rows\n",
    "    detail_info = list(df_temp['NER'])\n",
    "    \n",
    "    # Print output in desired format\n",
    "    print ('\\nIssue' + str(i) + ': ' + ' '.join(issues[i]))\n",
    "    print ('\\nEvent: ' + ', '.join(events))\n",
    "    for index in range(10):\n",
    "        print ('\\nDetail Information:')\n",
    "        print (events[index])\n",
    "        print ('\\t- ' + 'Person ({})'.format(', '.join(detail_info[index]['PERSON'])))\n",
    "        print ('\\t- ' + 'Organization ({})'.format(', '.join(detail_info[index]['ORG'])))\n",
    "        print ('\\t- ' + 'Places ({})'.format(', '.join(detail_info[index]['LOC'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
