{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Project on Korea Herald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 23769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "title          object\n",
       "author         object\n",
       "time           object\n",
       "description    object\n",
       "body           object\n",
       "section        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "data_path = '../data/koreaherald_1517_#.json.gz'\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "\n",
    "for i in range(8):\n",
    "  p = data_path.replace('#',str(i))\n",
    "  with gzip.open(p,'rb') as f:\n",
    "    data = pd.DataFrame.from_dict(json.load(f))\n",
    "  df_data = df_data.append(data,ignore_index=True)\n",
    "\n",
    "# clean up column names\n",
    "df_data = df_data.rename(columns={\" author\": \"author\",\n",
    "                        \" time\": \"time\",\n",
    "                        \" description\": \"description\",\n",
    "                        \" body\": \"body\",\n",
    "                        \" section\": \"section\",\n",
    "                       })\n",
    "# preview data\n",
    "print('Number of docs: {}'.format(df_data.shape[0]))\n",
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Here we apply:\n",
    "- tokenisation\n",
    "- lemmatisation\n",
    "- normalisation\n",
    "\n",
    "(optional) Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "whitelist = {'US':'United States'}\n",
    "\n",
    "def tokenise_pipeline(doc):\n",
    "  for k,v in whitelist:\n",
    "    doc = doc.replace(k,v)\n",
    "  doc = doc.lower()  # Convert to lowercase.\n",
    "  tokens = tokenizer.tokenize(doc) # split into words\n",
    "  # TODO: remove stopwords\n",
    "  tokens = [token for token in tokens if not token.isnumeric()] # remove numbers\n",
    "  tokens = [token for token in tokens if token not in stop_words]\n",
    "  tokens = [token for token in tokens if len(token) > 2] # remove words of only 1 letter\n",
    "  tokens = [lemmatizer.lemmatize(token) for token in tokens] # lemmatisation\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data['body_tokenised'] = df_data['body'].apply(tokenise_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "bigrams = Phrases(df_data['body_tokenised'].values, min_count=20) # keeps only phrases that appear >= 20 times in corpus.\n",
    "\n",
    "def add_bigrams(doc):\n",
    "  for token in bigrams[doc]:\n",
    "    if '_' in token:\n",
    "      doc.append(token)\n",
    "  return doc\n",
    "\n",
    "df_data['body_tokenised'] = df_data['body_tokenised'].apply(add_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 16614\n",
      "Number of documents: 23769\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(df_data['body_tokenised'].values)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5) # filter words that occur in less than 20 docs or more than 50% docs\n",
    "\n",
    "print('Number of unique tokens: {}'.format(len(dictionary)))\n",
    "print('Number of documents: {}'.format(len(df_data['body_tokenised'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# corpus = [word for doc in df_data['body_tokenised'] for word in doc]\n",
    "# vocab = list(set(corpus))\n",
    "# dictionary = pd.DataFrame(vocab,columns=['word'])\n",
    "# dictionary['doc_count'] = dictionary['word'].apply(lambda w: len(df_data[df_data['body_tokenised'].apply(lambda doc: w in doc)]))\n",
    "# print('Number of unique tokens: {}'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # filter words that occur in less than 20 docs or more than 50% docs\n",
    "# dictionary = dictionary[(dictionary['doc_count'] < 20) | (dictionary['doc_count'] > 1500)]\n",
    "# vocab = dictionary.word.values\n",
    "# df_data['body_tokenised'] = df_data['body_tokenised'].apply(lambda doc : list(filter(lambda w: w in vocab,doc)))\n",
    "# print('Number of unique tokens: {}'.format(len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data['body_tokenised'] = df_data['body_tokenised'].apply(lambda doc : ''.join(doc))\n",
    "# corpus = [doc for doc in df_data['body_tokenised']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import numpy as np\n",
    "\n",
    "# pipe = Pipeline([('count', CountVectorizer(vocabulary=vocab)),\n",
    "#                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "\n",
    "# df_data['body_vector'] = df_data['body_tokenised'].apply(lambda doc : np.squeeze(np.asarray(pipe.transform([doc]).todense())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW vectorisation\n",
    "# TFIDF vectorisation\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in df_data['body_tokenised']]\n",
    "tfidf = TfidfModel(corpus)\n",
    "df_data['body_bow'] = df_data['body_tokenised'].apply(dictionary.doc2bow)\n",
    "df_data['body_tfidf'] = df_data['body_bow'].apply(lambda doc : tfidf[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "df_2015 = copy.deepcopy(df_data[df_data['time'].str.contains('2015')])\n",
    "df_2016 = copy.deepcopy(df_data[df_data['time'].str.contains('2016')])\n",
    "df_2017 = copy.deepcopy(df_data[df_data['time'].str.contains('2017')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "temp = dictionary[0] # to 'load' dictionary\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "# parameters\n",
    "feature = 'body_bow' # lda uses bow\n",
    "corpus = df_2015[feature].values\n",
    "num_topics = 20\n",
    "\n",
    "model = LdaModel(\n",
    "  corpus = corpus,\n",
    "  id2word = id2word,\n",
    "  chunksize = 2000,\n",
    "  alpha = 'auto',\n",
    "  eta = 'auto',\n",
    "  iterations = 400,\n",
    "  num_topics = num_topics,\n",
    "  passes = 20,\n",
    "  eval_every = None\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4f8358df1bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyLDAvis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "from pyLDAvis import gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The larger the bubble, the more prevalent is that topic.\n",
    "# A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "# A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Topics to Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def argmax(ls):\n",
    "  return max(ls, key = lambda item: item[1])\n",
    "\n",
    "\n",
    "df_2015['topic'] = df_2015[feature].apply(lambda x : argmax(model.get_document_topics(x))[0])\n",
    "df_2015['topic_confidence'] = df_2015[feature].apply(lambda x : argmax(model.get_document_topics(x))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1: Number of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_2015.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 2: Duration of Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015[df_2015['topic'] == 1].time.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "report_duration = pd.DataFrame(columns=['topic','duration'])\n",
    "\n",
    "for i in range(num_topics):\n",
    "  df = df_2015[df_2015['topic'] == i]\n",
    "  max_t = datetime.strptime(df.time.max(), \"%Y-%m-%d %H:%M:%S\").date()\n",
    "  min_t = datetime.strptime(df.time.min(), \"%Y-%m-%d %H:%M:%S\").date()\n",
    "  report_duration = report_duration.append(pd.DataFrame({'topic': i, 'duration': (max_t - min_t)}, index=[i]))\n",
    "  \n",
    "report_duration.sort_values('duration',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 3: Entity Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = df_2015[df_2015['topic'] == 1].title.values\n",
    "\n",
    "entity_pairs = []\n",
    "\n",
    "for doc in topic_docs:\n",
    "  entity_pairs.append(get_entities(doc))\n",
    "  \n",
    "entity_pairs[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get n_topics by comparing Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from gensim.matutils import corpus2dense\n",
    "\n",
    "def get_silhouette_score(df,feature):\n",
    "  X = corpus2dense(df[feature].values, len(dictionary), num_docs=len(df)).T\n",
    "  y = df['topic'].values\n",
    "  return silhouette_score(X,y)\n",
    "\n",
    "get_silhouette_score(df_2015,'body_bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "temp = dictionary[0] # to 'load' dictionary\n",
    "id2word = dictionary.id2token\n",
    "feature = 'body_bow'\n",
    "\n",
    "X = df_2015[feature].values\n",
    "X_s = corpus2dense(X, len(dictionary), num_docs=len(X)).T\n",
    "\n",
    "def argmax(ls):\n",
    "  return max(ls, key = lambda item: item[1])\n",
    "\n",
    "\n",
    "for num_topics in range(10,61):\n",
    "  model = LdaModel(\n",
    "    corpus=X,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=None\n",
    "    )\n",
    "  y_s = df_2015[feature].apply(lambda x : argmax(model.get_document_topics(x))[0]).values\n",
    "  try:\n",
    "    score = silhouette_score(X_s,y_s)\n",
    "    print('Topic_size: {}, No. of labels: {}, Silhouette Score: {}'.format(num_topics, len(set(y_s)), score))\n",
    "  except:\n",
    "    print('Topic_size: {}, No. of labels: {},  Silhouette Score: {}'.format(num_topics, len(set(y_s)), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "cols = cols * 2\n",
    "\n",
    "max_words = 15\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=max_words,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = model.show_topics(formatted=False,num_topics=num_topics, num_words=max_words)\n",
    "\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
